---
format:
  html:
    code-overflow: wrap
---

# Defensive programming and generating tests

I find writing tests very boring. Its very necessary, but something I don't want to do regularly.

I've found that Claude 3.7 is pretty good at generating unit tests as well as pointing out how my code could be more defensive to prevent silent user errors ðŸž. Any AI can do this, but I've just found Claude 3.7 to be excellent. 

**BUT** Claude produced too much! It was too much for me for me to check.  So I've taken to using **system prompts** that prime Claude to respond in a way I can manage.


::: {.callout-important}
## Producivity boost, but care is still needed.

Even the best AI available makes mistakes when generating tests.  This could be due to your coding style(!), misunderstand the code, hallucination, a very long context etc. 

Overall I've found it to boost my productivity, and improve my test coverage. **But you have to check anything generated**.  
:::

::: {.callout-tip}
## System prompts

A system prompt is the first prompt you give to a generative AI (either directly or via some config prompt).  Use it to give the AI a "persona" to frame its responces in a certain way and context for how it should respond give certain prompts/commands.
:::

## 1. An example system prompt

This prompt basically sets Claude up as a Python testing expert and limits its responses to your commands.  It also sets you up to do few-shot prompt engineering where you provide your own data.

* **Task**: use the system prompt with your selected AI.

```markdown
## persona:
You are an expert software tester that specialises in defensive programming and unit testing of Python code using the package pytest. You will work with a user who will provide code and commands.

## defensive programming analysis:

Your first tasks when a user provides you with code are to:

1. analyse the code to understand functionality,
2. suggest defensive programming improvements

## unit tests:

To generate tests a user will provide the name of the function or class they wish to be tested. They will also specify a type of test they would like to be generated. This could be the following:

1. "functionality" - Check the code's core functionality
2. "edge" - Test extreme value and edge cases
3. "dirty" - Test that code fails as expected with certain values

For example a user may specify "foo functionality" where foo is the name of the function to test and functionality is the type of unit tests to create.

By default you will design the tests. But a user may optionality provide their test cases. A user may also issue the "restrict" command to limit testing to use the data they have specified.

For the type of unit test selected:

* Separate out tests that pytest will fail on based on your defensive programming analysis. This should not include dirty tests i.e. errors that are handled by exceptions implemented in the code already (dirty tests).
* Provide a summary of generated tests: this start with the number and then a list of each test name and what is is doing and how.
* Tests should be organised and easy for a user to understand. Make use of pytest functionality and decorators (e.g. pytest.approx and @pytest.mark.parametrize) to reduce redundant code.

If there is anything unclear or ambiguous with my request please report it. Otherwise confirm you have understood the instructions.
```

## 2. Test the bootstap code 

We will use the bootstrap code from a prior exercise. The docstring is generated by Gemini 2.5 Pro.

### 2.1 System prompt

Use our example system prompt, if you have not already done so.

### 2.2 Defensive code analysis

* **Option**: Add "this code is in a module called boostrap" to the prompt.
* **Task:** Promp the AI. Are there any changes you should consider?

```python
import numpy as np

def bootstrap(data, boots):
    """
    Generate bootstrap samples from the input data.
    
    This function creates multiple bootstrap samples by randomly sampling with
    replacement from the input data. Each bootstrap sample has the same size as
    the original dataset. The function then calculates the mean of each 
    bootstrap sample.
    
    Parameters
    ----------
    data : array-like
        The original data from which to generate bootstrap samples.
    boots : int
        The number of bootstrap samples to generate.
        
    Returns
    -------
    numpy.ndarray
        An array of bootstrap sample means with length equal to 'boots'.
        
    Examples
    --------
    >>> import numpy as np
    >>> original_data = [1, 2, 3, 4, 5]
    >>> bootstrap_means = bootstrap(original_data, 1000)
    >>> print(f"Original data mean: {np.mean(original_data)}")
    >>> print(f"Bootstrap means - 5th and 95th percentiles: {np.percentile(bootstrap_means, [5, 95])}")
    """
    # Convert input data to numpy array for efficient processing
    data = np.asarray(data)
    
    # Initialize random number generator
    rng = np.random.default_rng()
    
    # Generate bootstrap samples by randomly sampling with replacement
    # Creates a flattened array of (n_samples * boots) elements
    boot_data = data[rng.integers(0, data.shape[0], size=data.shape[0]*boots)]
    
    # Reshape the data to a 2D array with 'boots' rows 
    # calculate mean of each bootstrap sample
    return boot_data.reshape(-1, len(data)).sum(axis=1) / len(data)
```

### 2.3 Generate functionality tests

* **Task:** use the following prompt to generate bootstrap functionality. Do the tests make sense?

```markdown
bootstrap functionality
```

## 3. A more complex example

In this example we have a `mean_absolute_error` function. Similar to the functionality in `sklearn`. This version has a bit more input validation.  

**Task:** 

1. in a new context pass the system prompt.
2. pass the code (optionally specify it is in a module called metrics)
3. Review the defensive programming analysis (is my code rubbish or illogical?)
4. prompt your AI for dirty tests.


```python
def _convert_to_array(arr, name: str) -> np.ndarray:
    """
    Convert various input types to a 1D numpy array.
    
    Parameters:
    ----------
    arr : array-like or scalar
        The input to convert (DataFrame, Series, array, list, or scalar)
    name : str
        Name of the input for error messages
        
    Returns:
    -------
    np.ndarray
        Flattened 1D numpy array of float values
        
    Raises:
    ------
    TypeError
        If input cannot be converted to numeric array
    ValueError
        If input contains non-numeric values
    """
    # Check for multi-dimensional inputs and warn
    if isinstance(arr, pd.DataFrame) and arr.shape[1] > 1:
        warnings.warn(
            f"Multi-dimensional DataFrame provided for {name} with shape {arr.shape}. "
            "Only the flattened values will be used, which may not be what you intended.",
            UserWarning
        )
    
    if isinstance(arr, np.ndarray) and arr.ndim > 1:
        warnings.warn(
            f"Multi-dimensional array provided for {name} with shape {arr.shape}. "
            "Only the flattened values will be used, which may not be what you intended.",
            UserWarning
        )
    
    # Handle different input types
    if isinstance(arr, (pd.DataFrame, pd.Series)):
        arr = arr.to_numpy()
    elif not hasattr(arr, "__iter__") or isinstance(arr, (int, float)):
        arr = np.asarray([arr], dtype=float)
 
    # String checking
    if isinstance(arr, (str, bytes)):
        raise TypeError(f"String inputs are not supported: {arr}")

    try:
        return np.asarray(arr, dtype=float).flatten()
    except TypeError as e:
        raise TypeError(f"Cannot convert {name} to numeric array: {arr} - {str(e)}") from e
    except ValueError as e:
        raise ValueError(f"Failed to convert {name} to float array. Input contains non-numeric values: {str(e)}") from e

def _validate_single_array(arr: np.ndarray, name: str) -> np.ndarray:
    """
    Validate a single array for numeric content and basic quality.
    
    Parameters:
    ----------
    arr : np.ndarray
        The array to validate
    name : str
        Name of the array for error messages
        
    Returns:
    -------
    np.ndarray
        The validated array (unchanged)
        
    Raises:
    ------
    ValueError
        If array is empty, contains NaN, infinity values, or boolean values
    """
    # Check for empty arrays
    if len(arr) == 0:
        raise ValueError(f"{name} cannot be empty")

    # Check for boolean arrays
    if arr.dtype == bool or np.issubdtype(arr.dtype, np.bool_):
        raise ValueError(f"{name} contains boolean values. " \
            + "Please convert to numeric values (0 and 1) explicitly if intended.")
    
    # Check for NaN and infinity values
    if np.isnan(arr).any():
        raise ValueError(f"{name} contains NaN values")
    
    if np.isinf(arr).any():
        raise ValueError(f"{name} contains infinity values")
    
    # Check for zero arrays
    if np.all(arr == 0):
        warnings.warn(
            f"All values in {name} are zero, which may cause " \
                + "issues in percentage-based metrics",
            UserWarning
        )
    
    return arr


def _validate_inputs(
    y_true: npt.ArrayLike | int | float,
    y_pred: npt.ArrayLike | int | float
) -> tuple[np.ndarray, np.ndarray]:
    """
    Returns ground truth and predictions values as numpy arrays with enhanced validation.

    Parameters:
    --------
    y_true : array-like or scalar
        actual observations from time series
    y_pred : array-like or scalar
        the predictions

    Returns:
    -------
    Tuple(np.ndarray, np.ndarray)
        Validated and processed arrays

    Raises:
    ------
    ValueError
        If inputs have different lengths, are empty, contain invalid values
    TypeError
        If inputs cannot be converted to numeric types
    """
    # Step 1: Convert inputs to arrays
    y_true_arr = _convert_to_array(y_true, "y_true")
    y_pred_arr = _convert_to_array(y_pred, "y_pred")

    # Step 2: Validate each array individually
    y_true_arr = _validate_single_array(y_true_arr, "y_true")
    y_pred_arr = _validate_single_array(y_pred_arr, "y_pred")

    # Step 3: Perform pair-wise validations
    # check for same dimensions
    if len(y_true_arr) != len(y_pred_arr):
        raise ValueError(
            f"Input arrays must have the same length. Got {len(y_true_arr)} and {len(y_pred_arr)}"
        )
        
    # Check for very large differences that might indicate errors
    if np.max(np.abs(y_true_arr - y_pred_arr)) > 1e6:
        warnings.warn(
            "Very large differences detected between true and predicted values",
            UserWarning
        )

    return y_true_arr, y_pred_arr

    
def mean_absolute_error(
        y_true: npt.ArrayLike | int | float, 
        y_pred: npt.ArrayLike | int | float
) -> float:
    """
    Mean Absolute Error (MAE)

    Parameters:
    --------
    y_true -- array-like or int or float
        actual observations from time series
    y_pred -- array-like or int or float
        the predictions to evaluate

    Returns:
    -------
    float,
        scalar value representing the MAE

    Raises:
    ------
    ValueError
        If inputs cannot be converted to numeric arrays
    """
    y_true_arr, y_pred_arr = _validate_inputs(y_true, y_pred)
    return np.mean(np.abs((y_true_arr - y_pred_arr)))
```