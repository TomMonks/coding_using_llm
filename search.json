[
  {
    "objectID": "exercises/exercises.html",
    "href": "exercises/exercises.html",
    "title": "Basic Exercises",
    "section": "",
    "text": "A set of very basic prompt engineering examples. These focus on removing ambiguity from the prompts.\n\n\n\n\n\n\nChoose you favourite AI!\n\n\n\nChoose a generative AI ChatBot to use for this task. Some options are Perplexity.ai, Anthropic’s Claude, ChatGPT and Google Gemini.\n\n\n\n\n\n\n\nTask: Do you need to issue any additional prompts to fine tune the output? For example splitting the function into two separate functions representing the minimum and maximum values.\n\nGiven five positive integers, find the minimum and maximum values that can be calculated by summing exactly four of the five integers. Print the respective minimum and maximum values. Code the solution in Python as a python function that accepts a Python list as a parameter.\n\n\n\nIn one shot prompt engineering we are including some form of example of what we want in the context. For basic coding, one option is to include example data.\n\nTask: Modify this prompt using 1-shot prompt engineering i.e. include the example output. Try this in a new chat window and compare outputs.\n\nGiven five positive integers in a list, find the minimum and maximum values that can be calculated by summing exactly four of the five integers. Print the respective minimum and maximum values. Code the solution in Python as a python function that accepts a Python list as a parameter.\n\nExample \n\ninput  = [9, 3, 5, 7, 1]\nOutput = 16, 24\n\n\n\n\nThe prompts below are different approaches to generating the Python code for the basic bootrap.\n\n\n\n\n\n\nThe Bootstrap Method: A Simple Explanation\n\n\n\n\n\nThe bootstrap method is a statistical technique that helps estimate the properties of a sample (like its mean or variance) without making assumptions about how the data is distributed.\nImagine you have a small dataset and want to understand how reliable your calculated average (mean) is. The bootstrap works like this:\n\nTake your original data sample\nRandomly select values from it, with replacement (meaning you can pick the same value multiple times)\nCalculate the statistic of interest (like the mean) for this new “resampled” dataset\nRepeat steps 2-3 many times (typically hundreds or thousands)\nThe collection of calculated statistics forms a distribution that approximates how your statistic might vary across different samples\n\nThis approach lets you estimate confidence intervals and understand the uncertainty in your statistics without needing more data or making distributional assumptions. It’s particularly valuable when you have limited data or complex statistical situations.\n\n\n\n\n\n\nTask: Run the prompt below multiple times and observe the differences in code produced.\nAre there additional iterative prompts you can add to the context to improve the code or develop it towards your original intent?\n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean.\n\n\n\n\nThis time we provide additional detail to the generative tool.\nWe ask for efficiency, although this is vaguely specified\nWe list the function parameters and the function return value. Although again this could be argued to be ambiguous.\nTask: Run the prompt multiple time to see\n\n\n## Function description: \n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean. The function should make use of appropriate data science packages to ensure it is as efficient as possible.\n\n## Function parameters:\n\n1. An array-like object (e.g. numpy or a python list) that contains the original data.\n2. The number of bootstrap samples to obtain.\n\n## Return value:\nAn array-like object containing the bootstrap values\n\n\n\nThis time we include a less ambiguous specification I.e. use numpy and the default random number generator, use numpy vectorised operations etc. We will also added in a random seed to make the function reproducible and easier to test.\n\nTask: run the prompt in different contexts. How do the results differ? Has consistency improved? Do the results differ if you switch to an alternative ChatBot?\n\n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean.\n\n## Specification:\n1. Use numpy and its default random number generator\n2. Maximise the speed of the code by eliminating all python for loops and using calls to numpy universal functions.\n3. convert any array-like parameters to numpy arrays before performing the bootstrap\n\n## Function parameters:\n1. An array-like object (e.g. numpy or a python list) that contains the original data\n2. The number of bootstrap samples to take  (integer, default = 1000)\n3. A seed (int or SeedSequence) to ensure that the result of the resampling is reproducible. (default = None)\n\n## Return value: \nA 1 dimensional  numpy array object containing the bootstrap values.\n\n\n\nThis time we provide the code to the ChatBot and request that docstrings and comments are added.\n\nTask: Run the prompt - does the generated documentation make sense?\n\n\n## request:\nAdd a high quality PEP257 compliant docstring as well as code comments to the function below. Provide a usage example in the docstring and as code.\n\n## python code:\ndef bootstrap(data, boots):\n    data = np.asarray(data)\n    rng = np.random.default_rng()\n    boot_data = data[rng.integers(0, data.shape[0], size=data.shape[0]*boots)]\n    return boot_data.reshape(-1, len(data)).sum(axis=1) / len(data)"
  },
  {
    "objectID": "exercises/exercises.html#a-simple-python-function",
    "href": "exercises/exercises.html#a-simple-python-function",
    "title": "Basic Exercises",
    "section": "",
    "text": "Task: Do you need to issue any additional prompts to fine tune the output? For example splitting the function into two separate functions representing the minimum and maximum values.\n\nGiven five positive integers, find the minimum and maximum values that can be calculated by summing exactly four of the five integers. Print the respective minimum and maximum values. Code the solution in Python as a python function that accepts a Python list as a parameter.\n\n\n\nIn one shot prompt engineering we are including some form of example of what we want in the context. For basic coding, one option is to include example data.\n\nTask: Modify this prompt using 1-shot prompt engineering i.e. include the example output. Try this in a new chat window and compare outputs.\n\nGiven five positive integers in a list, find the minimum and maximum values that can be calculated by summing exactly four of the five integers. Print the respective minimum and maximum values. Code the solution in Python as a python function that accepts a Python list as a parameter.\n\nExample \n\ninput  = [9, 3, 5, 7, 1]\nOutput = 16, 24"
  },
  {
    "objectID": "exercises/exercises.html#the-bootstrap",
    "href": "exercises/exercises.html#the-bootstrap",
    "title": "Basic Exercises",
    "section": "",
    "text": "The prompts below are different approaches to generating the Python code for the basic bootrap.\n\n\n\n\n\n\nThe Bootstrap Method: A Simple Explanation\n\n\n\n\n\nThe bootstrap method is a statistical technique that helps estimate the properties of a sample (like its mean or variance) without making assumptions about how the data is distributed.\nImagine you have a small dataset and want to understand how reliable your calculated average (mean) is. The bootstrap works like this:\n\nTake your original data sample\nRandomly select values from it, with replacement (meaning you can pick the same value multiple times)\nCalculate the statistic of interest (like the mean) for this new “resampled” dataset\nRepeat steps 2-3 many times (typically hundreds or thousands)\nThe collection of calculated statistics forms a distribution that approximates how your statistic might vary across different samples\n\nThis approach lets you estimate confidence intervals and understand the uncertainty in your statistics without needing more data or making distributional assumptions. It’s particularly valuable when you have limited data or complex statistical situations.\n\n\n\n\n\n\nTask: Run the prompt below multiple times and observe the differences in code produced.\nAre there additional iterative prompts you can add to the context to improve the code or develop it towards your original intent?\n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean.\n\n\n\n\nThis time we provide additional detail to the generative tool.\nWe ask for efficiency, although this is vaguely specified\nWe list the function parameters and the function return value. Although again this could be argued to be ambiguous.\nTask: Run the prompt multiple time to see\n\n\n## Function description: \n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean. The function should make use of appropriate data science packages to ensure it is as efficient as possible.\n\n## Function parameters:\n\n1. An array-like object (e.g. numpy or a python list) that contains the original data.\n2. The number of bootstrap samples to obtain.\n\n## Return value:\nAn array-like object containing the bootstrap values\n\n\n\nThis time we include a less ambiguous specification I.e. use numpy and the default random number generator, use numpy vectorised operations etc. We will also added in a random seed to make the function reproducible and easier to test.\n\nTask: run the prompt in different contexts. How do the results differ? Has consistency improved? Do the results differ if you switch to an alternative ChatBot?\n\n\nwrite a python function that implements the basic bootstrap routine to construct the distribution of the mean.\n\n## Specification:\n1. Use numpy and its default random number generator\n2. Maximise the speed of the code by eliminating all python for loops and using calls to numpy universal functions.\n3. convert any array-like parameters to numpy arrays before performing the bootstrap\n\n## Function parameters:\n1. An array-like object (e.g. numpy or a python list) that contains the original data\n2. The number of bootstrap samples to take  (integer, default = 1000)\n3. A seed (int or SeedSequence) to ensure that the result of the resampling is reproducible. (default = None)\n\n## Return value: \nA 1 dimensional  numpy array object containing the bootstrap values.\n\n\n\nThis time we provide the code to the ChatBot and request that docstrings and comments are added.\n\nTask: Run the prompt - does the generated documentation make sense?\n\n\n## request:\nAdd a high quality PEP257 compliant docstring as well as code comments to the function below. Provide a usage example in the docstring and as code.\n\n## python code:\ndef bootstrap(data, boots):\n    data = np.asarray(data)\n    rng = np.random.default_rng()\n    boot_data = data[rng.integers(0, data.shape[0], size=data.shape[0]*boots)]\n    return boot_data.reshape(-1, len(data)).sum(axis=1) / len(data)"
  },
  {
    "objectID": "index.html#today",
    "href": "index.html#today",
    "title": "An introduction to prompt engineering for coding",
    "section": "Today",
    "text": "Today\n\nThe future 🔮🤩😬\nLLM contexts, context lengths and tokens\nHallucination 😵‍💫 vs. data contamination 🤢 vs. understanding 🤔\nRetrieval Augmented Generation (RAG)\nAI in your IDE versus a ChatBot\nApplied examples and exercises"
  },
  {
    "objectID": "index.html#exercises-for-fun",
    "href": "index.html#exercises-for-fun",
    "title": "An introduction to prompt engineering for coding",
    "section": "Exercises for fun",
    "text": "Exercises for fun\n\nOne and few-shot.\n\nCoding a basic boostrap\nAdding PEP257 or numpy style docstrings\n\nSystem prompt\n\ndefensive programming\nunits testing\n\n\n\nAll exercises were designed for my Python class 🐍. But please use what you like!"
  },
  {
    "objectID": "index.html#the-future",
    "href": "index.html#the-future",
    "title": "An introduction to prompt engineering for coding",
    "section": "The future",
    "text": "The future\n\nKwa et al. (2025). Measuring AI Ability to Complete Long Tasks. arxiv. https://arxiv.org/abs/2503.14499"
  },
  {
    "objectID": "index.html#context-and-tokens",
    "href": "index.html#context-and-tokens",
    "title": "An introduction to prompt engineering for coding",
    "section": "Context and tokens",
    "text": "Context and tokens\nGiven five positive integers, find the minimum and maximum values that \ncan be calculated by summing exactly four of the five integers. \nPrint the respective minimum and maximum values.\nToken visualiser: https://lunary.ai/deepseek-tokenizer"
  },
  {
    "objectID": "index.html#hallucination-1",
    "href": "index.html#hallucination-1",
    "title": "An introduction to prompt engineering for coding",
    "section": "Hallucination (1)",
    "text": "Hallucination (1)\n\nGoogle Bard launch marketing"
  },
  {
    "objectID": "index.html#hallucination-2",
    "href": "index.html#hallucination-2",
    "title": "An introduction to prompt engineering for coding",
    "section": "Hallucination (2)",
    "text": "Hallucination (2)\n\nNot a bot"
  },
  {
    "objectID": "index.html#prompt-engineering-for-coding",
    "href": "index.html#prompt-engineering-for-coding",
    "title": "An introduction to prompt engineering for coding",
    "section": "Prompt Engineering for coding",
    "text": "Prompt Engineering for coding\nThe process of crafting effective inputs to elicit desired outputs from LLMs."
  },
  {
    "objectID": "index.html#Exercises",
    "href": "index.html#Exercises",
    "title": "An introduction to prompt engineering for coding",
    "section": "",
    "text": "–&gt;"
  },
  {
    "objectID": "index.html#Testing",
    "href": "index.html#Testing",
    "title": "An introduction to prompt engineering for coding",
    "section": "",
    "text": "–&gt;\n\n\n\nhttps://github.com/TomMonks/coding_using_llm"
  },
  {
    "objectID": "exercises/testing_exercises.html",
    "href": "exercises/testing_exercises.html",
    "title": "Defensive programming and generating tests",
    "section": "",
    "text": "I find writing tests very boring. Its very necessary, but something I don’t want to do regularly.\nI’ve found that Claude 3.7 is pretty good at generating unit tests as well as pointing out how my code could be more defensive to prevent silent user errors 🐞. Any AI can do this, but I’ve just found Claude 3.7 to be excellent.\nBUT Claude produced too much! It was too much for me for me to check. So I’ve taken to using system prompts that prime Claude to respond in a way I can manage.\n\n\n\n\n\n\nProducivity boost, but care is still needed.\n\n\n\nEven the best AI available makes mistakes when generating tests. This could be due to your coding style(!), misunderstand the code, hallucination, a very long context etc.\nOverall I’ve found it to boost my productivity, and improve my test coverage. But you have to check anything generated.\n\n\n\n\n\n\n\n\nSystem prompts\n\n\n\nA system prompt is the first prompt you give to a generative AI (either directly or via some config prompt). Use it to give the AI a “persona” to frame its responces in a certain way and context for how it should respond give certain prompts/commands.\n\n\n\n\nThis prompt basically sets Claude up as a Python testing expert and limits its responses to your commands. It also sets you up to do few-shot prompt engineering where you provide your own data.\n\nTask: use the system prompt with your selected AI.\n\n## persona:\nYou are an expert software tester that specialises in defensive programming and unit testing of Python code using the package pytest. You will work with a user who will provide code and commands.\n\n## defensive programming analysis:\n\nYour first tasks when a user provides you with code are to:\n\n1. analyse the code to understand functionality,\n2. suggest defensive programming improvements\n\n## unit tests:\n\nTo generate tests a user will provide the name of the function or class they wish to be tested. They will also specify a type of test they would like to be generated. This could be the following:\n\n1. \"functionality\" - Check the code's core functionality\n2. \"edge\" - Test extreme value and edge cases\n3. \"dirty\" - Test that code fails as expected with certain values\n\nFor example a user may specify \"foo functionality\" where foo is the name of the function to test and functionality is the type of unit tests to create.\n\nBy default you will design the tests. But a user may optionality provide their test cases. A user may also issue the \"restrict\" command to limit testing to use the data they have specified.\n\nFor the type of unit test selected:\n\n* Separate out tests that pytest will fail on based on your defensive programming analysis. This should not include dirty tests i.e. errors that are handled by exceptions implemented in the code already (dirty tests).\n* Provide a summary of generated tests: this start with the number and then a list of each test name and what is is doing and how.\n* Tests should be organised and easy for a user to understand. Make use of pytest functionality and decorators (e.g. pytest.approx and @pytest.mark.parametrize) to reduce redundant code.\n\nIf there is anything unclear or ambiguous with my request please report it. Otherwise confirm you have understood the instructions.\n\n\n\nWe will use the bootstrap code from a prior exercise. The docstring is generated by Gemini 2.5 Pro.\n\n\nUse our example system prompt, if you have not already done so.\n\n\n\n\nOption: Add “this code is in a module called boostrap” to the prompt.\nTask: Promp the AI. Are there any changes you should consider?\n\nimport numpy as np\n\ndef bootstrap(data, boots):\n    \"\"\"\n    Generate bootstrap samples from the input data.\n    \n    This function creates multiple bootstrap samples by randomly sampling with\n    replacement from the input data. Each bootstrap sample has the same size as\n    the original dataset. The function then calculates the mean of each \n    bootstrap sample.\n    \n    Parameters\n    ----------\n    data : array-like\n        The original data from which to generate bootstrap samples.\n    boots : int\n        The number of bootstrap samples to generate.\n        \n    Returns\n    -------\n    numpy.ndarray\n        An array of bootstrap sample means with length equal to 'boots'.\n        \n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; original_data = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; bootstrap_means = bootstrap(original_data, 1000)\n    &gt;&gt;&gt; print(f\"Original data mean: {np.mean(original_data)}\")\n    &gt;&gt;&gt; print(f\"Bootstrap means - 5th and 95th percentiles: {np.percentile(bootstrap_means, [5, 95])}\")\n    \"\"\"\n    # Convert input data to numpy array for efficient processing\n    data = np.asarray(data)\n    \n    # Initialize random number generator\n    rng = np.random.default_rng()\n    \n    # Generate bootstrap samples by randomly sampling with replacement\n    # Creates a flattened array of (n_samples * boots) elements\n    boot_data = data[rng.integers(0, data.shape[0], size=data.shape[0]*boots)]\n    \n    # Reshape the data to a 2D array with 'boots' rows \n    # calculate mean of each bootstrap sample\n    return boot_data.reshape(-1, len(data)).sum(axis=1) / len(data)\n\n\n\n\nTask: use the following prompt to generate bootstrap functionality. Do the tests make sense?\n\nbootstrap functionality\n\n\n\n\nIn this example we have a mean_absolute_error function. Similar to the functionality in sklearn. This version has a bit more input validation.\nTask:\n\nin a new context pass the system prompt.\npass the code (optionally specify it is in a module called metrics)\nReview the defensive programming analysis (is my code rubbish or illogical?)\nprompt your AI for dirty tests.\n\ndef _convert_to_array(arr, name: str) -&gt; np.ndarray:\n    \"\"\"\n    Convert various input types to a 1D numpy array.\n    \n    Parameters:\n    ----------\n    arr : array-like or scalar\n        The input to convert (DataFrame, Series, array, list, or scalar)\n    name : str\n        Name of the input for error messages\n        \n    Returns:\n    -------\n    np.ndarray\n        Flattened 1D numpy array of float values\n        \n    Raises:\n    ------\n    TypeError\n        If input cannot be converted to numeric array\n    ValueError\n        If input contains non-numeric values\n    \"\"\"\n    # Check for multi-dimensional inputs and warn\n    if isinstance(arr, pd.DataFrame) and arr.shape[1] &gt; 1:\n        warnings.warn(\n            f\"Multi-dimensional DataFrame provided for {name} with shape {arr.shape}. \"\n            \"Only the flattened values will be used, which may not be what you intended.\",\n            UserWarning\n        )\n    \n    if isinstance(arr, np.ndarray) and arr.ndim &gt; 1:\n        warnings.warn(\n            f\"Multi-dimensional array provided for {name} with shape {arr.shape}. \"\n            \"Only the flattened values will be used, which may not be what you intended.\",\n            UserWarning\n        )\n    \n    # Handle different input types\n    if isinstance(arr, (pd.DataFrame, pd.Series)):\n        arr = arr.to_numpy()\n    elif not hasattr(arr, \"__iter__\") or isinstance(arr, (int, float)):\n        arr = np.asarray([arr], dtype=float)\n \n    # String checking\n    if isinstance(arr, (str, bytes)):\n        raise TypeError(f\"String inputs are not supported: {arr}\")\n\n    try:\n        return np.asarray(arr, dtype=float).flatten()\n    except TypeError as e:\n        raise TypeError(f\"Cannot convert {name} to numeric array: {arr} - {str(e)}\") from e\n    except ValueError as e:\n        raise ValueError(f\"Failed to convert {name} to float array. Input contains non-numeric values: {str(e)}\") from e\n\ndef _validate_single_array(arr: np.ndarray, name: str) -&gt; np.ndarray:\n    \"\"\"\n    Validate a single array for numeric content and basic quality.\n    \n    Parameters:\n    ----------\n    arr : np.ndarray\n        The array to validate\n    name : str\n        Name of the array for error messages\n        \n    Returns:\n    -------\n    np.ndarray\n        The validated array (unchanged)\n        \n    Raises:\n    ------\n    ValueError\n        If array is empty, contains NaN, infinity values, or boolean values\n    \"\"\"\n    # Check for empty arrays\n    if len(arr) == 0:\n        raise ValueError(f\"{name} cannot be empty\")\n\n    # Check for boolean arrays\n    if arr.dtype == bool or np.issubdtype(arr.dtype, np.bool_):\n        raise ValueError(f\"{name} contains boolean values. \" \\\n            + \"Please convert to numeric values (0 and 1) explicitly if intended.\")\n    \n    # Check for NaN and infinity values\n    if np.isnan(arr).any():\n        raise ValueError(f\"{name} contains NaN values\")\n    \n    if np.isinf(arr).any():\n        raise ValueError(f\"{name} contains infinity values\")\n    \n    # Check for zero arrays\n    if np.all(arr == 0):\n        warnings.warn(\n            f\"All values in {name} are zero, which may cause \" \\\n                + \"issues in percentage-based metrics\",\n            UserWarning\n        )\n    \n    return arr\n\n\ndef _validate_inputs(\n    y_true: npt.ArrayLike | int | float,\n    y_pred: npt.ArrayLike | int | float\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns ground truth and predictions values as numpy arrays with enhanced validation.\n\n    Parameters:\n    --------\n    y_true : array-like or scalar\n        actual observations from time series\n    y_pred : array-like or scalar\n        the predictions\n\n    Returns:\n    -------\n    Tuple(np.ndarray, np.ndarray)\n        Validated and processed arrays\n\n    Raises:\n    ------\n    ValueError\n        If inputs have different lengths, are empty, contain invalid values\n    TypeError\n        If inputs cannot be converted to numeric types\n    \"\"\"\n    # Step 1: Convert inputs to arrays\n    y_true_arr = _convert_to_array(y_true, \"y_true\")\n    y_pred_arr = _convert_to_array(y_pred, \"y_pred\")\n\n    # Step 2: Validate each array individually\n    y_true_arr = _validate_single_array(y_true_arr, \"y_true\")\n    y_pred_arr = _validate_single_array(y_pred_arr, \"y_pred\")\n\n    # Step 3: Perform pair-wise validations\n    # check for same dimensions\n    if len(y_true_arr) != len(y_pred_arr):\n        raise ValueError(\n            f\"Input arrays must have the same length. Got {len(y_true_arr)} and {len(y_pred_arr)}\"\n        )\n        \n    # Check for very large differences that might indicate errors\n    if np.max(np.abs(y_true_arr - y_pred_arr)) &gt; 1e6:\n        warnings.warn(\n            \"Very large differences detected between true and predicted values\",\n            UserWarning\n        )\n\n    return y_true_arr, y_pred_arr\n\n    \ndef mean_absolute_error(\n        y_true: npt.ArrayLike | int | float, \n        y_pred: npt.ArrayLike | int | float\n) -&gt; float:\n    \"\"\"\n    Mean Absolute Error (MAE)\n\n    Parameters:\n    --------\n    y_true -- array-like or int or float\n        actual observations from time series\n    y_pred -- array-like or int or float\n        the predictions to evaluate\n\n    Returns:\n    -------\n    float,\n        scalar value representing the MAE\n\n    Raises:\n    ------\n    ValueError\n        If inputs cannot be converted to numeric arrays\n    \"\"\"\n    y_true_arr, y_pred_arr = _validate_inputs(y_true, y_pred)\n    return np.mean(np.abs((y_true_arr - y_pred_arr)))"
  },
  {
    "objectID": "exercises/testing_exercises.html#an-example-system-prompt",
    "href": "exercises/testing_exercises.html#an-example-system-prompt",
    "title": "Defensive programming and generating tests",
    "section": "",
    "text": "This prompt basically sets Claude up as a Python testing expert and limits its responses to your commands. It also sets you up to do few-shot prompt engineering where you provide your own data.\n\nTask: use the system prompt with your selected AI.\n\n## persona:\nYou are an expert software tester that specialises in defensive programming and unit testing of Python code using the package pytest. You will work with a user who will provide code and commands.\n\n## defensive programming analysis:\n\nYour first tasks when a user provides you with code are to:\n\n1. analyse the code to understand functionality,\n2. suggest defensive programming improvements\n\n## unit tests:\n\nTo generate tests a user will provide the name of the function or class they wish to be tested. They will also specify a type of test they would like to be generated. This could be the following:\n\n1. \"functionality\" - Check the code's core functionality\n2. \"edge\" - Test extreme value and edge cases\n3. \"dirty\" - Test that code fails as expected with certain values\n\nFor example a user may specify \"foo functionality\" where foo is the name of the function to test and functionality is the type of unit tests to create.\n\nBy default you will design the tests. But a user may optionality provide their test cases. A user may also issue the \"restrict\" command to limit testing to use the data they have specified.\n\nFor the type of unit test selected:\n\n* Separate out tests that pytest will fail on based on your defensive programming analysis. This should not include dirty tests i.e. errors that are handled by exceptions implemented in the code already (dirty tests).\n* Provide a summary of generated tests: this start with the number and then a list of each test name and what is is doing and how.\n* Tests should be organised and easy for a user to understand. Make use of pytest functionality and decorators (e.g. pytest.approx and @pytest.mark.parametrize) to reduce redundant code.\n\nIf there is anything unclear or ambiguous with my request please report it. Otherwise confirm you have understood the instructions."
  },
  {
    "objectID": "exercises/testing_exercises.html#test-the-bootstap-code",
    "href": "exercises/testing_exercises.html#test-the-bootstap-code",
    "title": "Defensive programming and generating tests",
    "section": "",
    "text": "We will use the bootstrap code from a prior exercise. The docstring is generated by Gemini 2.5 Pro.\n\n\nUse our example system prompt, if you have not already done so.\n\n\n\n\nOption: Add “this code is in a module called boostrap” to the prompt.\nTask: Promp the AI. Are there any changes you should consider?\n\nimport numpy as np\n\ndef bootstrap(data, boots):\n    \"\"\"\n    Generate bootstrap samples from the input data.\n    \n    This function creates multiple bootstrap samples by randomly sampling with\n    replacement from the input data. Each bootstrap sample has the same size as\n    the original dataset. The function then calculates the mean of each \n    bootstrap sample.\n    \n    Parameters\n    ----------\n    data : array-like\n        The original data from which to generate bootstrap samples.\n    boots : int\n        The number of bootstrap samples to generate.\n        \n    Returns\n    -------\n    numpy.ndarray\n        An array of bootstrap sample means with length equal to 'boots'.\n        \n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; original_data = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; bootstrap_means = bootstrap(original_data, 1000)\n    &gt;&gt;&gt; print(f\"Original data mean: {np.mean(original_data)}\")\n    &gt;&gt;&gt; print(f\"Bootstrap means - 5th and 95th percentiles: {np.percentile(bootstrap_means, [5, 95])}\")\n    \"\"\"\n    # Convert input data to numpy array for efficient processing\n    data = np.asarray(data)\n    \n    # Initialize random number generator\n    rng = np.random.default_rng()\n    \n    # Generate bootstrap samples by randomly sampling with replacement\n    # Creates a flattened array of (n_samples * boots) elements\n    boot_data = data[rng.integers(0, data.shape[0], size=data.shape[0]*boots)]\n    \n    # Reshape the data to a 2D array with 'boots' rows \n    # calculate mean of each bootstrap sample\n    return boot_data.reshape(-1, len(data)).sum(axis=1) / len(data)\n\n\n\n\nTask: use the following prompt to generate bootstrap functionality. Do the tests make sense?\n\nbootstrap functionality"
  },
  {
    "objectID": "exercises/testing_exercises.html#a-more-complex-example",
    "href": "exercises/testing_exercises.html#a-more-complex-example",
    "title": "Defensive programming and generating tests",
    "section": "",
    "text": "In this example we have a mean_absolute_error function. Similar to the functionality in sklearn. This version has a bit more input validation.\nTask:\n\nin a new context pass the system prompt.\npass the code (optionally specify it is in a module called metrics)\nReview the defensive programming analysis (is my code rubbish or illogical?)\nprompt your AI for dirty tests.\n\ndef _convert_to_array(arr, name: str) -&gt; np.ndarray:\n    \"\"\"\n    Convert various input types to a 1D numpy array.\n    \n    Parameters:\n    ----------\n    arr : array-like or scalar\n        The input to convert (DataFrame, Series, array, list, or scalar)\n    name : str\n        Name of the input for error messages\n        \n    Returns:\n    -------\n    np.ndarray\n        Flattened 1D numpy array of float values\n        \n    Raises:\n    ------\n    TypeError\n        If input cannot be converted to numeric array\n    ValueError\n        If input contains non-numeric values\n    \"\"\"\n    # Check for multi-dimensional inputs and warn\n    if isinstance(arr, pd.DataFrame) and arr.shape[1] &gt; 1:\n        warnings.warn(\n            f\"Multi-dimensional DataFrame provided for {name} with shape {arr.shape}. \"\n            \"Only the flattened values will be used, which may not be what you intended.\",\n            UserWarning\n        )\n    \n    if isinstance(arr, np.ndarray) and arr.ndim &gt; 1:\n        warnings.warn(\n            f\"Multi-dimensional array provided for {name} with shape {arr.shape}. \"\n            \"Only the flattened values will be used, which may not be what you intended.\",\n            UserWarning\n        )\n    \n    # Handle different input types\n    if isinstance(arr, (pd.DataFrame, pd.Series)):\n        arr = arr.to_numpy()\n    elif not hasattr(arr, \"__iter__\") or isinstance(arr, (int, float)):\n        arr = np.asarray([arr], dtype=float)\n \n    # String checking\n    if isinstance(arr, (str, bytes)):\n        raise TypeError(f\"String inputs are not supported: {arr}\")\n\n    try:\n        return np.asarray(arr, dtype=float).flatten()\n    except TypeError as e:\n        raise TypeError(f\"Cannot convert {name} to numeric array: {arr} - {str(e)}\") from e\n    except ValueError as e:\n        raise ValueError(f\"Failed to convert {name} to float array. Input contains non-numeric values: {str(e)}\") from e\n\ndef _validate_single_array(arr: np.ndarray, name: str) -&gt; np.ndarray:\n    \"\"\"\n    Validate a single array for numeric content and basic quality.\n    \n    Parameters:\n    ----------\n    arr : np.ndarray\n        The array to validate\n    name : str\n        Name of the array for error messages\n        \n    Returns:\n    -------\n    np.ndarray\n        The validated array (unchanged)\n        \n    Raises:\n    ------\n    ValueError\n        If array is empty, contains NaN, infinity values, or boolean values\n    \"\"\"\n    # Check for empty arrays\n    if len(arr) == 0:\n        raise ValueError(f\"{name} cannot be empty\")\n\n    # Check for boolean arrays\n    if arr.dtype == bool or np.issubdtype(arr.dtype, np.bool_):\n        raise ValueError(f\"{name} contains boolean values. \" \\\n            + \"Please convert to numeric values (0 and 1) explicitly if intended.\")\n    \n    # Check for NaN and infinity values\n    if np.isnan(arr).any():\n        raise ValueError(f\"{name} contains NaN values\")\n    \n    if np.isinf(arr).any():\n        raise ValueError(f\"{name} contains infinity values\")\n    \n    # Check for zero arrays\n    if np.all(arr == 0):\n        warnings.warn(\n            f\"All values in {name} are zero, which may cause \" \\\n                + \"issues in percentage-based metrics\",\n            UserWarning\n        )\n    \n    return arr\n\n\ndef _validate_inputs(\n    y_true: npt.ArrayLike | int | float,\n    y_pred: npt.ArrayLike | int | float\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns ground truth and predictions values as numpy arrays with enhanced validation.\n\n    Parameters:\n    --------\n    y_true : array-like or scalar\n        actual observations from time series\n    y_pred : array-like or scalar\n        the predictions\n\n    Returns:\n    -------\n    Tuple(np.ndarray, np.ndarray)\n        Validated and processed arrays\n\n    Raises:\n    ------\n    ValueError\n        If inputs have different lengths, are empty, contain invalid values\n    TypeError\n        If inputs cannot be converted to numeric types\n    \"\"\"\n    # Step 1: Convert inputs to arrays\n    y_true_arr = _convert_to_array(y_true, \"y_true\")\n    y_pred_arr = _convert_to_array(y_pred, \"y_pred\")\n\n    # Step 2: Validate each array individually\n    y_true_arr = _validate_single_array(y_true_arr, \"y_true\")\n    y_pred_arr = _validate_single_array(y_pred_arr, \"y_pred\")\n\n    # Step 3: Perform pair-wise validations\n    # check for same dimensions\n    if len(y_true_arr) != len(y_pred_arr):\n        raise ValueError(\n            f\"Input arrays must have the same length. Got {len(y_true_arr)} and {len(y_pred_arr)}\"\n        )\n        \n    # Check for very large differences that might indicate errors\n    if np.max(np.abs(y_true_arr - y_pred_arr)) &gt; 1e6:\n        warnings.warn(\n            \"Very large differences detected between true and predicted values\",\n            UserWarning\n        )\n\n    return y_true_arr, y_pred_arr\n\n    \ndef mean_absolute_error(\n        y_true: npt.ArrayLike | int | float, \n        y_pred: npt.ArrayLike | int | float\n) -&gt; float:\n    \"\"\"\n    Mean Absolute Error (MAE)\n\n    Parameters:\n    --------\n    y_true -- array-like or int or float\n        actual observations from time series\n    y_pred -- array-like or int or float\n        the predictions to evaluate\n\n    Returns:\n    -------\n    float,\n        scalar value representing the MAE\n\n    Raises:\n    ------\n    ValueError\n        If inputs cannot be converted to numeric arrays\n    \"\"\"\n    y_true_arr, y_pred_arr = _validate_inputs(y_true, y_pred)\n    return np.mean(np.abs((y_true_arr - y_pred_arr)))"
  }
]